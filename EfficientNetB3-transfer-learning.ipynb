{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#! pip install keras pandas efficientnet sklearn scikit-image opencv-python\n",
    "#! apt-get install -y libsm6 libxext6 libxrender-dev\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from efficientnet import EfficientNetB3, center_crop_and_resize, preprocess_input\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEPATH = \"stanford-dogs-dataset/\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=1,restore_best_weights=True, monitor=\"val_acc\")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(factor=0.1, patience=3,verbose=1, monitor=\"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(conv_base, trainable=False, dense_layers=2):\n",
    "    model = models.Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    for _ in range(dense_layers):\n",
    "        model.add(layers.Dense(2048))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(512))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(120,activation='softmax'))\n",
    "    \n",
    "    if not trainable:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    model.compile(\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b3 (Model)      (None, 7, 7, 1536)        10783528  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 11,632,032\n",
      "Trainable params: 848,504\n",
      "Non-trainable params: 10,783,528\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "conv_base=EfficientNetB3(weights='imagenet',include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "model = build_model(conv_base,dense_layers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255.0,\n",
    "                                   shear_range=0.1,\n",
    "                                   rotation_range=10.,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   zoom_range=[0.9, 1.25],\n",
    "                                   brightness_range=[0.8, 1.2],\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15394 images belonging to 120 classes.\n",
      "Found 1114 images belonging to 120 classes.\n",
      "Found 4072 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    BASEPATH + \"train\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    BASEPATH + \"test\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    BASEPATH + \"val\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "481/481 [==============================] - 384s 798ms/step - loss: 2.6635 - acc: 0.3683 - val_loss: 1.2165 - val_acc: 0.6631\n",
      "Epoch 2/100\n",
      "481/481 [==============================] - 364s 756ms/step - loss: 1.9597 - acc: 0.4940 - val_loss: 1.1655 - val_acc: 0.6733\n",
      "Epoch 3/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.8855 - acc: 0.5097 - val_loss: 1.1466 - val_acc: 0.6822\n",
      "Epoch 4/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.8424 - acc: 0.5226 - val_loss: 1.1659 - val_acc: 0.6760\n",
      "Epoch 5/100\n",
      "481/481 [==============================] - 362s 752ms/step - loss: 1.7916 - acc: 0.5355 - val_loss: 1.1612 - val_acc: 0.6832\n",
      "Epoch 6/100\n",
      "481/481 [==============================] - 364s 756ms/step - loss: 1.7570 - acc: 0.5468 - val_loss: 1.1685 - val_acc: 0.6809\n",
      "Epoch 7/100\n",
      "481/481 [==============================] - 363s 755ms/step - loss: 1.7523 - acc: 0.5467 - val_loss: 1.1522 - val_acc: 0.6891\n",
      "Epoch 8/100\n",
      "481/481 [==============================] - 361s 751ms/step - loss: 1.7505 - acc: 0.5512 - val_loss: 1.1422 - val_acc: 0.6864\n",
      "Epoch 9/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.7035 - acc: 0.5565 - val_loss: 1.1616 - val_acc: 0.6827\n",
      "Epoch 10/100\n",
      "481/481 [==============================] - 362s 754ms/step - loss: 1.7048 - acc: 0.5630 - val_loss: 1.1757 - val_acc: 0.6829\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/100\n",
      "481/481 [==============================] - 361s 750ms/step - loss: 1.5025 - acc: 0.5953 - val_loss: 1.1376 - val_acc: 0.6948\n",
      "Epoch 12/100\n",
      "481/481 [==============================] - 362s 754ms/step - loss: 1.4488 - acc: 0.6084 - val_loss: 1.1139 - val_acc: 0.6985\n",
      "Epoch 13/100\n",
      "481/481 [==============================] - 362s 752ms/step - loss: 1.4540 - acc: 0.6086 - val_loss: 1.0924 - val_acc: 0.7030\n",
      "Epoch 14/100\n",
      "481/481 [==============================] - 362s 752ms/step - loss: 1.3927 - acc: 0.6160 - val_loss: 1.0977 - val_acc: 0.7027\n",
      "Epoch 15/100\n",
      "481/481 [==============================] - 364s 756ms/step - loss: 1.3832 - acc: 0.6192 - val_loss: 1.0854 - val_acc: 0.7025\n",
      "Epoch 16/100\n",
      "481/481 [==============================] - 363s 754ms/step - loss: 1.3800 - acc: 0.6184 - val_loss: 1.0800 - val_acc: 0.6998\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 17/100\n",
      "481/481 [==============================] - 362s 752ms/step - loss: 1.3265 - acc: 0.6357 - val_loss: 1.1053 - val_acc: 0.6936\n",
      "Epoch 18/100\n",
      "481/481 [==============================] - 361s 751ms/step - loss: 1.3457 - acc: 0.6355 - val_loss: 1.0746 - val_acc: 0.7022\n",
      "Epoch 19/100\n",
      "481/481 [==============================] - 364s 757ms/step - loss: 1.3313 - acc: 0.6339 - val_loss: 1.0676 - val_acc: 0.7054\n",
      "Epoch 20/100\n",
      "481/481 [==============================] - 363s 754ms/step - loss: 1.3436 - acc: 0.6284 - val_loss: 1.1077 - val_acc: 0.6963\n",
      "Epoch 21/100\n",
      "481/481 [==============================] - 362s 752ms/step - loss: 1.3242 - acc: 0.6340 - val_loss: 1.0381 - val_acc: 0.7094\n",
      "Epoch 22/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.3342 - acc: 0.6335 - val_loss: 1.1028 - val_acc: 0.7062\n",
      "Epoch 23/100\n",
      "481/481 [==============================] - 363s 755ms/step - loss: 1.3193 - acc: 0.6351 - val_loss: 1.0603 - val_acc: 0.7017\n",
      "Epoch 24/100\n",
      "481/481 [==============================] - 363s 756ms/step - loss: 1.3242 - acc: 0.6330 - val_loss: 1.0952 - val_acc: 0.6990\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 25/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.3251 - acc: 0.6323 - val_loss: 1.0692 - val_acc: 0.7106\n",
      "Epoch 26/100\n",
      "481/481 [==============================] - 360s 749ms/step - loss: 1.3262 - acc: 0.6352 - val_loss: 1.0883 - val_acc: 0.6985\n",
      "Epoch 27/100\n",
      "481/481 [==============================] - 363s 755ms/step - loss: 1.3052 - acc: 0.6389 - val_loss: 1.0978 - val_acc: 0.6988\n",
      "Epoch 28/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.3137 - acc: 0.6404 - val_loss: 1.0398 - val_acc: 0.7124\n",
      "Epoch 29/100\n",
      "481/481 [==============================] - 362s 752ms/step - loss: 1.3313 - acc: 0.6327 - val_loss: 1.1048 - val_acc: 0.6963\n",
      "Epoch 30/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.3311 - acc: 0.6324 - val_loss: 1.0575 - val_acc: 0.7030\n",
      "Epoch 31/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.3131 - acc: 0.6407 - val_loss: 1.1128 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 32/100\n",
      "481/481 [==============================] - 363s 754ms/step - loss: 1.3248 - acc: 0.6381 - val_loss: 1.0755 - val_acc: 0.7040\n",
      "Epoch 33/100\n",
      "481/481 [==============================] - 362s 752ms/step - loss: 1.3289 - acc: 0.6366 - val_loss: 1.0786 - val_acc: 0.7017\n",
      "Epoch 34/100\n",
      "481/481 [==============================] - 362s 753ms/step - loss: 1.3277 - acc: 0.6337 - val_loss: 1.0729 - val_acc: 0.7045\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 35/100\n",
      "481/481 [==============================] - 361s 750ms/step - loss: 1.3336 - acc: 0.6323 - val_loss: 1.1027 - val_acc: 0.6903\n",
      "Epoch 36/100\n",
      "481/481 [==============================] - 360s 749ms/step - loss: 1.3032 - acc: 0.6418 - val_loss: 1.0697 - val_acc: 0.7025\n",
      "Epoch 37/100\n",
      "481/481 [==============================] - 363s 754ms/step - loss: 1.3248 - acc: 0.6348 - val_loss: 1.0731 - val_acc: 0.7087\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 38/100\n",
      "481/481 [==============================] - 361s 751ms/step - loss: 1.3136 - acc: 0.6392 - val_loss: 1.0613 - val_acc: 0.7020\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f37b793af28>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // BATCH_SIZE,\n",
    "    epochs = EPOCHS, callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate_generator(test_generator,verbose=0, steps=test_generator.samples // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0938490585369223 0.6939338235294118\n"
     ]
    }
   ],
   "source": [
    "print(loss,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"efficientnetb3-model-{:.2f}acc-{:.2f}loss-{:.0f}.hdf5\".format(acc, loss, time.time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
