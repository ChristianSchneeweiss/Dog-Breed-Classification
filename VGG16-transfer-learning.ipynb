{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I found this dataset rather cute so I tried to make a deep learning model to predict the breed of the dog\n",
    "\n",
    "I used transfer learning, basically I took an already trained model and added layers to it, and trained it on this dataset.\n",
    "\n",
    "The model I chose is the VGG16, trained on the Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the images and preprocessing\n",
    "\n",
    "I chose to load all images into memory, but if you don't have enough memory (around 9.3GB) you can use the generator from keras https://keras.io/preprocessing/image/.\n",
    "\n",
    "Further we have to preprocess the image to fit the VGG16 model. The model expects a 224 by 224 RGB image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEPATH = \"../input/images/Images/\"\n",
    "\n",
    "LABELS = set()\n",
    "\n",
    "paths = []\n",
    "    \n",
    "for d in os.listdir(BASEPATH):\n",
    "    LABELS.add(d)\n",
    "    paths.append((BASEPATH+d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# resizing and converting to RGB\n",
    "def load_and_preprocess_image(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (224,224))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for path, label in paths:\n",
    "    for image_path in os.listdir(path):\n",
    "        image = load_and_preprocess_image(path+\"/\"+image_path)\n",
    "        \n",
    "        X.append(image)\n",
    "        y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels\n",
    "\n",
    "Every model I know of, can only work with numbers, therefore we need to create our label (n02096294-Australian_terrier) to an integer array. This is called **one-hot-encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()\n",
    "\n",
    "X = np.array(X)\n",
    "y = encoder.fit_transform(np.array(y))\n",
    "\n",
    "print(X[0].dtype)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "plt.imshow(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating\n",
    "\n",
    "Now comes to most interesting part. Training and evaluating our model\n",
    "\n",
    "First step is to create a training and testing dataset\n",
    "\n",
    "After this I have 2 approaches.\n",
    "- Creating our own model, which I tried\n",
    "- Using transfer learning on the VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the own model I tried. I have to say I did not have a great succes with it. Maybe it needs more finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(len(LABELS),activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the transfer learning model\n",
    "\n",
    "I used the VGG16 with the Imagenet weights and did not include the top, so I can add my own top. Being 3 Dense Layers with relu activation, and the last one with softmax and number of neurons is the amount of labels we have.\n",
    "\n",
    "I set the last 5 layers to trainable, which are the Dense layers I added myself. I don't want to train any layers I get from the pretrained VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 3s 0us/step\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 16,875,960\n",
      "Trainable params: 2,161,272\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "base_model=VGG16(weights='imagenet',include_top=False)\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x)\n",
    "x=Dense(1024,activation='relu')(x)\n",
    "x=Dropout(0.5)(x)\n",
    "x=Dense(512,activation='relu')(x)\n",
    "preds=Dense(len(LABELS),activation='softmax')(x)\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "for layer in model.layers[:-5]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[-5:]:\n",
    "    layer.trainable=True\n",
    "    \n",
    "model.compile(\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, verbose=1,restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=3,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "I used the Adam optimizer, typically my first choice.\n",
    "\n",
    "I trained the model on 50 epochs, with a batch size of 64, I used two callbacks. Early stopping to stop the training of the validation loss does not improve for 5 epochs, and Reduce LR on plateau to reduce the learning rate to 10% if the validation loss doesn't improve for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,batch_size=64,epochs=50,validation_data=(X_test,y_test), callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Last step should always be to evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test,y_test,verbose=0)\n",
    "print(f\"loss on the test set is {loss:.2f}\")\n",
    "print(f\"accuracy on the test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy on the test set is over 50% maybe by tuning the model and the hyperparameters we could increase this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predictions = encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 5, 3\n",
    "size = 25\n",
    "\n",
    "fig,ax=plt.subplots(rows,cols)\n",
    "fig.set_size_inches(size,size)\n",
    "for i in range(rows):\n",
    "    for j in range (cols):\n",
    "        index = np.random.randint(0,len(X_test))\n",
    "        ax[i,j].imshow(X_test[index])\n",
    "        ax[i,j].set_title(f'Predicted: {label_predictions[index]}\\n Actually: {encoder.inverse_transform(y_test)[index]}')\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the ImageDataGenerator\n",
    "\n",
    "Now I tried the results with the ImageDataGenerator. This allows us to scale the pixels from a value between 0-255 down to 0-1, this usually works better for deep learning algorithms. Furthermore I added two image generation methods, zooming and horizontal flip. This increases the dataset and therefore allows us to work with more data and get better results. Hopefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255.0,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16508 images belonging to 120 classes.\n",
      "Found 4072 images belonging to 120 classes.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 62s 2s/step - loss: 4.3841 - acc: 0.0324\n",
      "129/129 [==============================] - 274s 2s/step - loss: 4.6999 - acc: 0.0159 - val_loss: 4.3841 - val_acc: 0.0324\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 60s 2s/step - loss: 3.8544 - acc: 0.0911\n",
      "129/129 [==============================] - 253s 2s/step - loss: 4.1870 - acc: 0.0512 - val_loss: 3.8544 - val_acc: 0.0911\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 60s 2s/step - loss: 3.5909 - acc: 0.1316\n",
      "129/129 [==============================] - 251s 2s/step - loss: 3.6940 - acc: 0.1084 - val_loss: 3.5909 - val_acc: 0.1316\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 3.3818 - acc: 0.1606\n",
      "129/129 [==============================] - 250s 2s/step - loss: 3.4369 - acc: 0.1443 - val_loss: 3.3818 - val_acc: 0.1606\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 3.2430 - acc: 0.1928\n",
      "129/129 [==============================] - 250s 2s/step - loss: 3.2535 - acc: 0.1788 - val_loss: 3.2430 - val_acc: 0.1928\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 3.1359 - acc: 0.2168\n",
      "129/129 [==============================] - 249s 2s/step - loss: 3.1294 - acc: 0.1992 - val_loss: 3.1359 - val_acc: 0.2168\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 3.0825 - acc: 0.2333\n",
      "129/129 [==============================] - 249s 2s/step - loss: 3.0345 - acc: 0.2252 - val_loss: 3.0825 - val_acc: 0.2333\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 58s 2s/step - loss: 3.0148 - acc: 0.2343\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.9437 - acc: 0.2419 - val_loss: 3.0148 - val_acc: 0.2343\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.9647 - acc: 0.2456\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.8605 - acc: 0.2556 - val_loss: 2.9647 - val_acc: 0.2456\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.9564 - acc: 0.2525\n",
      "129/129 [==============================] - 249s 2s/step - loss: 2.7923 - acc: 0.2701 - val_loss: 2.9564 - val_acc: 0.2525\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.9223 - acc: 0.2645\n",
      "129/129 [==============================] - 249s 2s/step - loss: 2.7361 - acc: 0.2841 - val_loss: 2.9223 - val_acc: 0.2645\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.9085 - acc: 0.2711\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.6780 - acc: 0.2914 - val_loss: 2.9085 - val_acc: 0.2711\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.8360 - acc: 0.2758\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.6468 - acc: 0.2996 - val_loss: 2.8360 - val_acc: 0.2758\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.8280 - acc: 0.2778\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.5925 - acc: 0.3111 - val_loss: 2.8280 - val_acc: 0.2778\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.8688 - acc: 0.2721\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.5433 - acc: 0.3237 - val_loss: 2.8688 - val_acc: 0.2721\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.8184 - acc: 0.2832\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.5153 - acc: 0.3309 - val_loss: 2.8184 - val_acc: 0.2832\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.8042 - acc: 0.2908\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.4705 - acc: 0.3381 - val_loss: 2.8042 - val_acc: 0.2908\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.8495 - acc: 0.2800\n",
      "129/129 [==============================] - 245s 2s/step - loss: 2.4299 - acc: 0.3458 - val_loss: 2.8495 - val_acc: 0.2800\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.8405 - acc: 0.2841\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.4023 - acc: 0.3530 - val_loss: 2.8405 - val_acc: 0.2841\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.7609 - acc: 0.2986\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.3570 - acc: 0.3627 - val_loss: 2.7609 - val_acc: 0.2986\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.8144 - acc: 0.2915\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.3387 - acc: 0.3674 - val_loss: 2.8144 - val_acc: 0.2915\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.7549 - acc: 0.3102\n",
      "129/129 [==============================] - 249s 2s/step - loss: 2.2912 - acc: 0.3735 - val_loss: 2.7549 - val_acc: 0.3102\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 59s 2s/step - loss: 2.7673 - acc: 0.3048\n",
      "129/129 [==============================] - 248s 2s/step - loss: 2.2638 - acc: 0.3805 - val_loss: 2.7673 - val_acc: 0.3048\n",
      "Epoch 24/50\n",
      " 50/129 [==========>...................] - ETA: 1:40 - loss: 2.1791 - acc: 0.3973"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    BASEPATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    BASEPATH, # same directory as training data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = 50, callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test,y_test,verbose=0)\n",
    "print(f\"loss on the test set is {loss:.2f}\")\n",
    "print(f\"accuracy on the test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "label_predictions = encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 5, 3\n",
    "size = 25\n",
    "\n",
    "fig,ax=plt.subplots(rows,cols)\n",
    "fig.set_size_inches(size,size)\n",
    "for i in range(rows):\n",
    "    for j in range (cols):\n",
    "        index = np.random.randint(0,len(X_test))\n",
    "        ax[i,j].imshow(X_test[index])\n",
    "        ax[i,j].set_title(f'Predicted: {label_predictions[index]}\\n Actually: {encoder.inverse_transform(y_test)[index]}')\n",
    "        \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
